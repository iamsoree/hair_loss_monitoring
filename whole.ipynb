{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa22dbb0",
   "metadata": {},
   "source": [
    "# **탈모 진행 단계별 모니터링 시스템;**\n",
    "### **두피와 머리카락 분석을 통한 추적 관리 및 예측 모델링**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db04e699",
   "metadata": {},
   "source": [
    "##### **Ⅰ. 모델 개요**\n",
    "##### **Ⅱ. 모델 구축**\n",
    "##### **Ⅲ. (DL) 모델 경량화**\n",
    "##### **Ⅳ. 모델 배포**&ensp;(실시간 처리 방식 / 이미지 업로드 방식)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5fff67",
   "metadata": {},
   "source": [
    "### **Ⅰ. 모델 개요**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26bd820",
   "metadata": {},
   "source": [
    "##### **`FUNCTION 1` : 탈모 상태 분류 (4개)**\n",
    "&ensp;&ensp;✓&ensp;DL&ensp;&rarr;&ensp;<u>CNN</u>  \n",
    "&ensp;&ensp;✓&ensp;MODEL&ensp;&rarr;&ensp;<u>EFFICIENTNET_V2_S</u>  \n",
    "&ensp;&ensp;✓&ensp;경량화\n",
    "\n",
    "##### **`FUNCTION 2` : 두피 면적과 머리카락 면적의 비율**\n",
    "&ensp;&ensp;✓&ensp;ML&ensp;&rarr;&ensp;<u>Color Segmentation</u>  \n",
    "&ensp;&ensp;✓&ensp;MODEL&ensp;&rarr;&ensp;<u>K-MEANS CLUSTERING</u>\n",
    "\n",
    "##### **`FUNCTION 3` : 모공 하나 당 머리카락 개수**\n",
    "&ensp;&ensp;✓&ensp;DL&ensp;&rarr;&ensp;<u>Object Detection</u>  \n",
    "&ensp;&ensp;✓&ensp;MODEL&ensp;&rarr;&ensp;<u>YOLO 11N</u>  \n",
    "&ensp;&ensp;✓&ensp;경량화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c1a2b",
   "metadata": {},
   "source": [
    "### **Ⅱ. 모델 구축**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690c8af",
   "metadata": {},
   "source": [
    "#### **(FUNCTION 1) EFFICIENTNET_V2_S**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a575113",
   "metadata": {},
   "source": [
    "##### **환경 _ 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3657c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6577062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(15)\n",
    "\n",
    "g = torch.Generator().manual_seed(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1f6e65",
   "metadata": {},
   "source": [
    "##### **데이터 _ 준비**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb31a487",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834586a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('condition'):\n",
    "    path = '/content/drive/MyDrive/dataset/hair_condition.zip'\n",
    "    f_zip = zipfile.ZipFile(path)\n",
    "    f_zip.extractall('condition')\n",
    "    f_zip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c17424",
   "metadata": {},
   "source": [
    "##### **데이터 _ 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((img_height, img_width)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness = 0.2),\n",
    "    transforms.RandomRotation(degrees = (0, 180)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((img_height, img_width)),\n",
    "    transforms.CenterCrop((img_height, img_width)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, transform = None):\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        root = Path(img_dir)\n",
    "        self.paths = list(root.glob('*/*'))\n",
    "\n",
    "        self.class_name = ['normal', 'mild', 'moderate', 'severe']\n",
    "        self.cindex = {'normal' : 0, 'mild' : 1, 'moderate' : 2, 'severe' : 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        path = self.paths[idx]\n",
    "        label = path.parts[-2]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.cindex[label]\n",
    "\n",
    "train_set = CustomImageDataset('./condition/training', transform = train_transforms)\n",
    "val_set = CustomImageDataset('./condition/testing', transform = val_transforms)\n",
    "\n",
    "class_names = train_set.class_name\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size = batch_size, shuffle = True, generator = g)\n",
    "val_loader = DataLoader(val_set, batch_size = batch_size, shuffle = False, generator = g)\n",
    "\n",
    "for feature, label in train_loader:\n",
    "\n",
    "    print(feature.shape)\n",
    "\n",
    "    for i in range(4):\n",
    "        \n",
    "        ax = plt.subplot(2, 2, i + 1)\n",
    "\n",
    "        plt.imshow(feature[i].permute((1, 2, 0)))\n",
    "        plt.title(class_names[label[i]])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232028b5",
   "metadata": {},
   "source": [
    "##### **모델 _ 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.efficientnet_v2_s(weights = 'IMAGENET1K_V1')\n",
    "\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EFCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes = 4, dropout = 0.3):\n",
    "\n",
    "        super(EFCNN, self).__init__()\n",
    "\n",
    "        self.model = model = models.efficientnet_v2_s(weights = 'IMAGENET1K_V1')\n",
    "        nn_feature = self.model.classifier[1].in_features\n",
    "        self.model.classifier = nn.Identity()\n",
    "\n",
    "        self.fc1 = nn.Linear(nn_feature, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.bn2 = nn.BatchNorm1d(num_classes)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.model(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f485712c",
   "metadata": {},
   "source": [
    "##### **모델 _ 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309ccac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'hlc_model'\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EFCNN(dropout = 0.5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.00001, weight_decay = 1e-4)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.5, patience = 5, verbose = True)\n",
    "\n",
    "if not os.path.isdir('/checkpoint'):\n",
    "    os.mkdir('/checkpoint')\n",
    "\n",
    "history = {'train_loss' : [], 'val_loss' : [], 'train_acc' : [], 'val_acc' : []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    t_loss = 0\n",
    "    t_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (outputs.argmax(dim = 1) == labels).sum() / len(labels)\n",
    "\n",
    "        t_loss += loss.item()\n",
    "        t_acc += acc\n",
    "\n",
    "        if i % 10 == 0:\n",
    "          print('.', end = '')\n",
    "\n",
    "    print(f\"Epoch : {epoch}, Loss : {t_loss / (i+1)}, Accuracy : {t_acc / (i+1)}\")\n",
    "\n",
    "    history['train_loss'].append(t_loss / len(train_loader))\n",
    "    history['train_acc'].append((t_acc / len(train_loader)).cpu().numpy())\n",
    "\n",
    "    checkpoint_path = f\"/checkpoint/{model_name}_cp{epoch}.pt\"\n",
    "\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "    v_loss = 0\n",
    "    v_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (images, labels) in enumerate(val_loader):\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            acc = (outputs.argmax(dim = 1) == labels).sum() / len(labels)\n",
    "\n",
    "            v_acc += acc\n",
    "            v_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch : {epoch}, Val_Loss : {v_loss / (i+1)}, Val_Accuracy : {v_acc / (i+1)}\")\n",
    "\n",
    "        history['val_loss'].append(v_loss / len(val_loader))\n",
    "        history['val_acc'].append((v_acc / len(val_loader)).cpu().numpy())\n",
    "\n",
    "        scheduler.step(v_loss / len(val_loader))\n",
    "\n",
    "os.makedirs('ptmodel', exist_ok = True)\n",
    "torch.save(model, f\"ptmodel/{model_name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d032780",
   "metadata": {},
   "source": [
    "##### **모델 _ 검증**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13852522",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history['train_loss']\n",
    "acc = history['train_acc']\n",
    "val_loss = history['val_loss']\n",
    "val_acc = history['val_acc']\n",
    "\n",
    "eps = range(len(val_loss))\n",
    "\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(eps, loss, label = 'train_loss')\n",
    "ax1.plot(eps, val_loss, label = 'val_loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(eps, acc, label = 'train_acc')\n",
    "ax2.plot(eps, val_acc, label = 'val_acc')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc_epoch = np.argmax(history['val_acc'])\n",
    "\n",
    "best_val_acc = history['val_acc'][best_val_acc_epoch]\n",
    "best_val_loss = history['val_loss'][best_val_acc_epoch]\n",
    "\n",
    "print(f\"Best Epoch For Validation Accuracy : {best_val_acc_epoch}\")\n",
    "print(f\"Best Validation Accuracy : {best_val_acc}\")\n",
    "print(f\"Best Validation Loss : {best_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6223ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss_epoch = np.argmin(history['val_loss'])\n",
    "\n",
    "best_val_loss = history['val_loss'][best_val_loss_epoch]\n",
    "best_val_acc = history['val_acc'][best_val_loss_epoch]\n",
    "\n",
    "print(f\"Best Epoch For Validation Loss : {best_val_loss_epoch}\")\n",
    "print(f\"Best Validation Loss : {best_val_loss}\")\n",
    "print(f\"Best Validation Accuracy : {best_val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_acc = (history['val_acc'] - np.min(history['val_acc'])) / (np.max(history['val_acc']) - np.min(history['val_acc']))\n",
    "normalized_loss = (history['val_loss'] - np.min(history['val_loss'])) / (np.max(history['val_loss']) - np.min(history['val_loss']))\n",
    "\n",
    "score = normalized_acc + (1 - normalized_loss)\n",
    "\n",
    "best_epoch = np.argmax(score)\n",
    "\n",
    "best_val_acc = history['val_acc'][best_epoch]\n",
    "best_val_loss = history['val_loss'][best_epoch]\n",
    "\n",
    "print(f\"Best Epoch (Considering Both) : {best_epoch}\")\n",
    "print(f\"Best Validation Accuracy : {best_val_acc}\")\n",
    "print(f\"Best Validation Loss : {best_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad52e6c0",
   "metadata": {},
   "source": [
    "##### **모델 _ 저장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c04d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download('/content/ptmodel/hlc_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f981570",
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download('/checkpoint/hlc_model_cp92.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2c98db",
   "metadata": {},
   "source": [
    "#### **(FUNCTION 2) K-MEANS CLUSTERING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5a8670",
   "metadata": {},
   "source": [
    "##### **환경 _ 설정**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d17b0",
   "metadata": {},
   "source": [
    "##### **데이터 _ 준비**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c727e78e",
   "metadata": {},
   "source": [
    "##### **데이터 _ 전처리**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb63adc",
   "metadata": {},
   "source": [
    "##### **모델 _ 정의**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb7436",
   "metadata": {},
   "source": [
    "##### **모델 _ 학습**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53a124f",
   "metadata": {},
   "source": [
    "##### **모델 _ 평가**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d27be94",
   "metadata": {},
   "source": [
    "#### **(FUNCTION 3) YOLO11N**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f22a4a",
   "metadata": {},
   "source": [
    "##### **환경 _ 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d3e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bae9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bc3edb",
   "metadata": {},
   "source": [
    "##### **데이터 _ 준비**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541a6cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13144327",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('hairloss_data'):\n",
    "  path = '/content/drive/MyDrive/dataset/hairloss.zip'\n",
    "  f_zip = zipfile.ZipFile(path)\n",
    "  f_zip.extractall('hairloss_data')\n",
    "  f_zip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceace6cc",
   "metadata": {},
   "source": [
    "##### **모델 _ 정의 & 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "model.train(data = 'hairloss_data/hairloss.yaml', epochs = 300, project = 'hairloss_od', name = 'hairloss_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f786eaf6",
   "metadata": {},
   "source": [
    "##### **모델 _ 저장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17027732",
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download('hairloss_od/hairloss_model/weights/best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c5a368",
   "metadata": {},
   "source": [
    "##### **모델 _ 추론**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3020c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"hairloss_od/hairloss_model/weights/best.pt\")\n",
    "\n",
    "root = Path(\"hairloss_data/Validation/images\")\n",
    "paths = list(root.glob('*'))\n",
    "\n",
    "plt.figure(figsize = (20, 20))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for path in paths[:9]:\n",
    "  \n",
    "  results = model(path)\n",
    "  im_array = results[0].plot()\n",
    "  im = Image.fromarray(im_array[..., ::-1])\n",
    "\n",
    "  i += 1\n",
    "\n",
    "  ax = plt.subplot(3, 3, i)\n",
    "  ax.imshow(im)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ddb9a4",
   "metadata": {},
   "source": [
    "### **Ⅲ. (DL) 모델 경량화**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb2e18d",
   "metadata": {},
   "source": [
    "#### **(FUNCTION 1) EFFICIENTNET_V2_S**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e411aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sng4onnx onnx onnxruntime onnx_graphsurgeon onnxslim onnx2tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89c0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a019fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5623f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/checkpoint/cphlc_model92.pt', map_location = 'cpu'))\n",
    "model = model.to('cpu')\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "print(dummy_input)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "torch.onnx.export(model, dummy_input, \"/hlc_model.onnx\", export_params = True, opset_version = 18, input_names = ['input'], output_names = ['output'], dynamic_axes = {'input' : {0 : 'batch_size'}, 'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a67d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!onnx2tf -i /hlc_model.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download(\"saved_model/hlc_model_float16.tflite\")\n",
    "files.download(\"saved_model/hlc_model_float32.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb19b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"/content/drive/MyDrive/saved_model\")\n",
    "\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS\n",
    "]\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "converter.experimental_new_converter = True\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open(\"hlcM_torchQ16.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "files.download(\"hlcM_torchQ16.tflite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52004229",
   "metadata": {},
   "source": [
    "#### **(FUNCTION 3) YOLO11N**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21247904",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics onnx2tf onnx onnxruntime tflite_support 'sng4onnx>=1.0.1', 'onnx_graphsurgeon>=0.3.26', 'onnx2tf>1.17.5,<=1.26.3', 'onnxslim>=0.1.31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc53855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28dece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"best.pt\")\n",
    "\n",
    "model.export(format = 'tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download('best_saved_model/best_float32.tflite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4e3f3",
   "metadata": {},
   "source": [
    "### **Ⅳ. 모델 배포**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5539174",
   "metadata": {},
   "source": [
    "#### **1. 실시간 처리 방식 (BY Raspberry Pi)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fce75b",
   "metadata": {},
   "source": [
    "##### **1-1. FUNCTION 1 + FUNCTION 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017fbf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tflite_runtime.interpreter as tflite\n",
    "import time\n",
    "import threading\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "classification_names = ['normal', 'mild', 'moderate', 'severe']\n",
    "detection_names = ['1hair', '2hair', '3hair', '4hair']\n",
    "\n",
    "classification_model_path = 'hlcM_torchQ16.tflite'\n",
    "detection_model_path = 'yolo300_float32.tflite'\n",
    "\n",
    "classification_interpreter = tflite.Interpreter(model_path = classification_model_path)\n",
    "classification_interpreter.allocate_tensors()\n",
    "class_input_details = classification_interpreter.get_input_details()\n",
    "class_output_details = classification_interpreter.get_output_details()\n",
    "\n",
    "detection_interpreter = tflite.Interpreter(model_path = detection_model_path)\n",
    "detection_interpreter.allocate_tensors()\n",
    "detect_input_details = detection_interpreter.get_input_details()\n",
    "detect_output_details = detection_interpreter.get_output_details()\n",
    "\n",
    "detect_img_height = detect_input_details[0]['shape'][1]\n",
    "detect_img_width = detect_input_details[0]['shape'][2]\n",
    "\n",
    "def iou(box1, box2):\n",
    "\n",
    "    x1_min, y1_min, x1_max, y1_max = (\n",
    "        box1[0] - box1[2] / 2, box1[1] - box1[3] / 2,\n",
    "        box1[0] + box1[2] / 2, box1[1] + box1[3] / 2\n",
    "    )\n",
    "    x2_min, y2_min, x2_max, y2_max = (\n",
    "        box2[0] - box2[2] / 2, box2[1] - box2[3] / 2,\n",
    "        box2[0] + box2[2] / 2, box2[1] + box2[3] / 2\n",
    "    )\n",
    "\n",
    "    inter_x1 = max(x1_min, x2_min)\n",
    "    inter_y1 = max(y1_min, y2_min)\n",
    "    inter_x2 = min(x1_max, x2_max)\n",
    "    inter_y2 = min(y1_max, y2_max)\n",
    "\n",
    "    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "    box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "def merge_boxes(boxes, scores, classes, iou_threshold = 0.4):\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, scores, score_threshold = 0.2, nms_threshold = iou_threshold)\n",
    "    \n",
    "    merged_result = []\n",
    "\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            merged_result.append((np.array(boxes[i]), scores[i], classes[i]))\n",
    "    \n",
    "    return merged_result\n",
    "\n",
    "def imgproc(img_q, result_q):\n",
    "    while True:\n",
    "\n",
    "        if not img_q:\n",
    "            continue\n",
    "\n",
    "        image = img_q.popleft()\n",
    "\n",
    "        if image is None:\n",
    "            break\n",
    "\n",
    "        image_resized = cv2.resize(image, (detect_img_width, detect_img_height))\n",
    "        image_resized = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        image_np = np.array(image_resized, dtype = np.float32) / 255.0\n",
    "        image_np = np.expand_dims(image_np, axis = 0)\n",
    "\n",
    "        detection_interpreter.set_tensor(detect_input_details[0]['index'], image_np)\n",
    "        detection_interpreter.invoke()\n",
    "\n",
    "        output = detection_interpreter.get_tensor(detect_output_details[0]['index'])[0].T\n",
    "\n",
    "        boxes_xywh = output[:, :4]\n",
    "        scores = np.max(output[:, 4:], axis = 1)\n",
    "        classes = np.argmax(output[:, 4:], axis = 1)\n",
    "        \n",
    "        threshold = 0.2\n",
    "\n",
    "        filtered_boxes, filtered_scores, filtered_classes = [], [], []\n",
    "        \n",
    "        for i, s in enumerate(scores):\n",
    "            if s > threshold:\n",
    "                filtered_boxes.append(boxes_xywh[i].tolist())\n",
    "                filtered_scores.append(float(s))\n",
    "                filtered_classes.append(int(classes[i]))\n",
    "\n",
    "        merged_result = merge_boxes(filtered_boxes, filtered_scores, filtered_classes)\n",
    "        result_q.append(merged_result)\n",
    "\n",
    "        time.sleep(0.01)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "_, frame = cap.read()\n",
    "h, w, c = frame.shape\n",
    "cen, ofs = w // 2, h // 2\n",
    "left, right = cen - ofs, cen + ofs\n",
    "\n",
    "img_q, result_q = deque(maxlen = 3), deque(maxlen = 10)\n",
    "\n",
    "detect_thread = threading.Thread(target = imgproc, args = (img_q, result_q))\n",
    "detect_thread.start()\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    image = frame[:, left:right]\n",
    "    img_q.append(image)\n",
    "    \n",
    "    class_image = cv2.resize(image, (224, 224))\n",
    "    class_image = cv2.cvtColor(class_image, cv2.COLOR_BGR2RGB)\n",
    "    class_image = np.array(class_image, dtype = np.float32)\n",
    "    class_image = np.expand_dims(class_image, axis = 0)\n",
    "    \n",
    "    classification_interpreter.set_tensor(class_input_details[0]['index'], class_image)\n",
    "    classification_interpreter.invoke()\n",
    "    class_output = classification_interpreter.get_tensor(class_output_details[0]['index'])\n",
    "    class_pred = np.argmax(class_output[0])\n",
    "    class_label = classification_names[class_pred]\n",
    "    \n",
    "    detection_colors = {0 : (255, 0, 0), 1 : (0, 255, 0), 2 : (0, 0, 255), 3 : (255, 255, 0)}\n",
    "    \n",
    "    if result_q:\n",
    "\n",
    "        detected_objects = result_q.popleft()\n",
    "\n",
    "        for box, score, cls in detected_objects:\n",
    "            \n",
    "            box = np.array(box)\n",
    "\n",
    "            x_center, y_center, width, height = box * np.array([w, h, w, h])\n",
    "\n",
    "            x1 = int(x_center - width / 2)\n",
    "            y1 = int(y_center - height / 2)\n",
    "            x2 = int(x_center + width / 2)\n",
    "            y2 = int(y_center + height / 2)\n",
    "        \n",
    "            color = detection_colors.get(cls, (255, 255, 255))\n",
    "\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "            text = f\"{detection_names[cls]} ({score : .2f})\"\n",
    "            \n",
    "            cv2.putText(image, text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)    \n",
    "\n",
    "    cv2.putText(image, f\"HL Condition : {class_label}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 165, 255), 2)\n",
    "    \n",
    "    image_resized = cv2.resize(image, (w * 2, h * 2))\n",
    "\n",
    "    cv2.imshow('FUNCTION13', image_resized)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord('q'):\n",
    "        img_q.append(None)\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab5155a",
   "metadata": {},
   "source": [
    "##### **1-2. FUNCTION 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc8bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"카메라에서 프레임을 읽을 수 없습니다.\")\n",
    "        break\n",
    "    \n",
    "    frame_resized = cv2.resize(frame, (320, 240))\n",
    "    image_lab = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2LAB)\n",
    "    pixels = image_lab.reshape((-1, 3))\n",
    "    kmeans = KMeans(n_clusters = 2, random_state = 45, n_init = 10)\n",
    "    labels = kmeans.fit_predict(pixels)\n",
    "    segmented_image = labels.reshape(frame_resized.shape[:2])\n",
    "\n",
    "    hair_pixels = np.sum(labels == 0)\n",
    "    scalp_pixels = np.sum(labels == 1)\n",
    "    total_pixels = hair_pixels + scalp_pixels\n",
    "\n",
    "    hair_ratio = (hair_pixels / total_pixels) * 100\n",
    "    scalp_ratio = (scalp_pixels / total_pixels) * 100\n",
    "\n",
    "    segmented_colored = np.zeros_like(frame_resized)\n",
    "    segmented_colored[segmented_image == 0] = [0, 0, 0]    # 머리카락\n",
    "    segmented_colored[segmented_image == 1] = [255, 255, 255]    # 두피\n",
    "\n",
    "    text = f\"Hair : {hair_ratio : .2f}% | Scalp : {scalp_ratio : .2f}%\"\n",
    "    \n",
    "    cv2.putText(segmented_colored, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"FUNCTION2\", segmented_colored)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8ab2a6",
   "metadata": {},
   "source": [
    "#### **2. 이미지 업로드 방식 (BY Flask)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cec97e",
   "metadata": {},
   "source": [
    "##### **2-1. FRONT-END**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d6502",
   "metadata": {},
   "source": [
    "##### **2-2. BACK-END**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
